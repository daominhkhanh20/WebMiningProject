{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../EDA/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_lable = {1:0,2:0,3:2,4:1,5:1}\n",
    "data['lable'] = data['rating_star'].map(map_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:20000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('../../assets/stopword/stopword.txt','r',encoding=\"utf8\") as f:\n",
    "    for word in f:\n",
    "        stopwords.append(word.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(data.comment[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(), data['lable'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(ANNModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.drop_out = 0.25\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "                nn.Linear(self.input_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.drop_out),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.drop_out),\n",
    "\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.drop_out),\n",
    "\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.drop_out),\n",
    "\n",
    "                nn.Linear(128, 10),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=self.drop_out),\n",
    "\n",
    "                nn.Linear(10, self.output_size),\n",
    "                nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size,output_size = X_train.shape[1],len(y_train.unique())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ANNModel(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANNModel(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=11407, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.25, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.25, inplace=False)\n",
       "    (12): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.25, inplace=False)\n",
       "    (15): Linear(in_features=10, out_features=3, bias=True)\n",
       "    (16): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(data,y,batch_size):\n",
    "    X = torch.tensor(data)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(X,y)\n",
    "    loader = DataLoader(dataset, batch_size= batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = make_loader(X_train,y_train.values,batch_size)\n",
    "test_loader = make_loader(X_test,y_test.values,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    # print(f'Training epoch {epoch}...')\n",
    "    running_loss,total,correct = 0.0, 0, 0\n",
    "    for ix, batch in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        X = batch[0].to(device)\n",
    "        y = torch.reshape(batch[1], (-1,))\n",
    "        y = y.type(torch.LongTensor).to(device)\n",
    "        outputs = model(X.float())\n",
    "\n",
    "        loss = criterion(outputs.float(), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        predict = torch.argmax(outputs, dim=1)\n",
    "        total += predict.size(0)\n",
    "        correct += (predict == y).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(running_loss,correct/sample_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe4737cf0a41cf5e93c7bf4b9dc2d43d3c69fc85d82c68e3039db569ea2744e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
